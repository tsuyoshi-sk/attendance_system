#!/usr/bin/env python3
"""
å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
NFCå‹¤æ€ ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®ç¶™ç¶šçš„å“è³ªç›£è¦–
"""

import asyncio
import json
import time
import os
import psutil
import subprocess
from datetime import datetime, timedelta
from typing import Dict, Any, List
import aiohttp
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict


class QualityMonitoringSystem:
    """å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.metrics_history = defaultdict(list)
        self.alert_thresholds = {
            'code_coverage': 95.0,
            'test_pass_rate': 100.0,
            'response_time_p95': 500.0,  # ms
            'error_rate': 1.0,  # %
            'cpu_usage': 80.0,  # %
            'memory_usage': 85.0,  # %
            'security_score': 90.0
        }
        self.monitoring_interval = 300  # 5åˆ†
        self.is_monitoring = False
        
    async def start_continuous_monitoring(self):
        """ç¶™ç¶šçš„å“è³ªç›£è¦–é–‹å§‹"""
        print("ğŸš€ å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
        self.is_monitoring = True
        
        try:
            while self.is_monitoring:
                print(f"\nğŸ“Š å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†é–‹å§‹ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                # å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                metrics = await self.collect_all_metrics()
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ä¿å­˜
                self.store_metrics(metrics)
                
                # å“è³ªåˆ†æ
                analysis = await self.analyze_quality_trends()
                
                # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
                alerts = await self.check_quality_gates(metrics, analysis)
                
                if alerts:
                    await self.send_quality_alerts(alerts)
                    
                # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°
                await self.update_quality_dashboard(metrics, analysis)
                
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                await self.generate_realtime_report(metrics, analysis)
                
                print(f"âœ… ç›£è¦–ã‚µã‚¤ã‚¯ãƒ«å®Œäº† - æ¬¡å›: {(datetime.now() + timedelta(seconds=self.monitoring_interval)).strftime('%H:%M:%S')}")
                
                # æ¬¡ã®ç›£è¦–ã‚µã‚¤ã‚¯ãƒ«ã¾ã§å¾…æ©Ÿ
                await asyncio.sleep(self.monitoring_interval)
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢")
            self.is_monitoring = False
            
    async def collect_all_metrics(self) -> Dict[str, Any]:
        """å…¨å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        print("  ğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ä¸­...")
        
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'code_quality': await self.analyze_code_quality(),
            'test_coverage': await self.get_test_coverage(),
            'test_results': await self.get_test_results(),
            'performance': await self.measure_system_performance(),
            'security': await self.run_security_scan(),
            'system_health': await self.check_system_health(),
            'error_metrics': await self.analyze_error_rates(),
            'user_experience': await self.measure_user_experience()
        }
        
        return metrics
        
    async def analyze_code_quality(self) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ"""
        try:
            # Pylintå®Ÿè¡Œ
            pylint_result = subprocess.run(
                ['pylint', '--output-format=json', 'app/'],
                capture_output=True,
                text=True
            )
            
            if pylint_result.stdout:
                pylint_data = json.loads(pylint_result.stdout)
                total_issues = len(pylint_data)
                
                issue_types = defaultdict(int)
                for issue in pylint_data:
                    issue_types[issue.get('type', 'unknown')] += 1
                    
                # è¤‡é›‘åº¦åˆ†æï¼ˆRadonä½¿ç”¨ï¼‰
                complexity_result = subprocess.run(
                    ['radon', 'cc', 'app/', '-j'],
                    capture_output=True,
                    text=True
                )
                
                complexity_data = {}
                if complexity_result.stdout:
                    complexity_json = json.loads(complexity_result.stdout)
                    total_complexity = 0
                    function_count = 0
                    
                    for file_path, functions in complexity_json.items():
                        for func in functions:
                            total_complexity += func.get('complexity', 0)
                            function_count += 1
                            
                    complexity_data = {
                        'average_complexity': total_complexity / function_count if function_count > 0 else 0,
                        'high_complexity_functions': sum(1 for f in functions if f.get('complexity', 0) > 10)
                    }
                    
                return {
                    'pylint_score': 10.0 - (total_issues * 0.1),  # ç°¡æ˜“ã‚¹ã‚³ã‚¢è¨ˆç®—
                    'total_issues': total_issues,
                    'issue_breakdown': dict(issue_types),
                    'complexity': complexity_data,
                    'technical_debt_hours': total_issues * 0.5  # æ¨å®šæŠ€è¡“çš„è² å‚µæ™‚é–“
                }
                
        except Exception as e:
            print(f"    âš ï¸ ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        return {
            'pylint_score': 0,
            'total_issues': 0,
            'issue_breakdown': {},
            'complexity': {},
            'technical_debt_hours': 0
        }
        
    async def get_test_coverage(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸å–å¾—"""
        try:
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆå®Ÿè¡Œ
            coverage_result = subprocess.run(
                ['coverage', 'report', '--format=json'],
                capture_output=True,
                text=True
            )
            
            if coverage_result.stdout:
                coverage_data = json.loads(coverage_result.stdout)
                
                return {
                    'overall_coverage': coverage_data.get('totals', {}).get('percent_covered', 0),
                    'lines_covered': coverage_data.get('totals', {}).get('covered_lines', 0),
                    'lines_missing': coverage_data.get('totals', {}).get('missing_lines', 0),
                    'files_analyzed': len(coverage_data.get('files', {})),
                    'low_coverage_files': [
                        file for file, data in coverage_data.get('files', {}).items()
                        if data.get('summary', {}).get('percent_covered', 100) < 80
                    ]
                }
                
        except Exception as e:
            print(f"    âš ï¸ ã‚«ãƒãƒ¬ãƒƒã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        return {
            'overall_coverage': 0,
            'lines_covered': 0,
            'lines_missing': 0,
            'files_analyzed': 0,
            'low_coverage_files': []
        }
        
    async def get_test_results(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœå–å¾—"""
        try:
            # pytestå®Ÿè¡Œï¼ˆå‰å›ã®çµæœã‚’ä½¿ç”¨ï¼‰
            if os.path.exists('test_results.json'):
                with open('test_results.json', 'r') as f:
                    test_data = json.load(f)
                    
                return {
                    'total_tests': test_data.get('total', 0),
                    'passed_tests': test_data.get('passed', 0),
                    'failed_tests': test_data.get('failed', 0),
                    'skipped_tests': test_data.get('skipped', 0),
                    'test_pass_rate': (test_data.get('passed', 0) / test_data.get('total', 1)) * 100,
                    'execution_time': test_data.get('duration', 0),
                    'failed_test_names': test_data.get('failed_tests', [])
                }
                
        except Exception as e:
            print(f"    âš ï¸ ãƒ†ã‚¹ãƒˆçµæœå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        return {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'skipped_tests': 0,
            'test_pass_rate': 0,
            'execution_time': 0,
            'failed_test_names': []
        }
        
    async def measure_system_performance(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
        performance_metrics = {
            'response_times': [],
            'throughput': 0,
            'error_count': 0,
            'latency_percentiles': {}
        }
        
        try:
            # APIå¿œç­”æ™‚é–“æ¸¬å®š
            async with aiohttp.ClientSession() as session:
                response_times = []
                
                for _ in range(10):  # 10å›ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    start_time = time.time()
                    
                    try:
                        async with session.get(f"{self.base_url}/health", timeout=5) as response:
                            response_time = (time.time() - start_time) * 1000
                            response_times.append(response_time)
                            
                            if response.status != 200:
                                performance_metrics['error_count'] += 1
                                
                    except Exception:
                        performance_metrics['error_count'] += 1
                        
                    await asyncio.sleep(0.1)
                    
                if response_times:
                    response_times.sort()
                    performance_metrics['response_times'] = response_times
                    performance_metrics['latency_percentiles'] = {
                        'p50': response_times[len(response_times) // 2],
                        'p95': response_times[int(len(response_times) * 0.95)],
                        'p99': response_times[int(len(response_times) * 0.99)] if len(response_times) > 10 else response_times[-1]
                    }
                    performance_metrics['throughput'] = len(response_times) / (sum(response_times) / 1000)
                    
        except Exception as e:
            print(f"    âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        return performance_metrics
        
    async def run_security_scan(self) -> Dict[str, Any]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œ"""
        security_metrics = {
            'vulnerabilities': [],
            'security_score': 100,
            'dependency_vulnerabilities': 0,
            'code_vulnerabilities': 0
        }
        
        try:
            # Banditã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³
            bandit_result = subprocess.run(
                ['bandit', '-r', 'app/', '-f', 'json'],
                capture_output=True,
                text=True
            )
            
            if bandit_result.stdout:
                bandit_data = json.loads(bandit_result.stdout)
                issues = bandit_data.get('results', [])
                
                for issue in issues:
                    security_metrics['vulnerabilities'].append({
                        'severity': issue.get('issue_severity'),
                        'confidence': issue.get('issue_confidence'),
                        'description': issue.get('issue_text'),
                        'filename': issue.get('filename'),
                        'line': issue.get('line_number')
                    })
                    
                    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢æ¸›ç‚¹
                    if issue.get('issue_severity') == 'HIGH':
                        security_metrics['security_score'] -= 10
                    elif issue.get('issue_severity') == 'MEDIUM':
                        security_metrics['security_score'] -= 5
                    elif issue.get('issue_severity') == 'LOW':
                        security_metrics['security_score'] -= 2
                        
                security_metrics['code_vulnerabilities'] = len(issues)
                
            # Safety ã«ã‚ˆã‚‹ä¾å­˜é–¢ä¿‚è„†å¼±æ€§ãƒã‚§ãƒƒã‚¯
            safety_result = subprocess.run(
                ['safety', 'check', '--json'],
                capture_output=True,
                text=True
            )
            
            if safety_result.stdout:
                safety_data = json.loads(safety_result.stdout)
                security_metrics['dependency_vulnerabilities'] = len(safety_data)
                security_metrics['security_score'] -= len(safety_data) * 5
                
        except Exception as e:
            print(f"    âš ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        security_metrics['security_score'] = max(0, security_metrics['security_score'])
        
        return security_metrics
        
    async def check_system_health(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        health_metrics = {
            'cpu_usage': psutil.cpu_percent(interval=1),
            'memory_usage': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'active_connections': 0,
            'process_count': len(psutil.pids()),
            'system_load': os.getloadavg() if hasattr(os, 'getloadavg') else [0, 0, 0]
        }
        
        try:
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šæ•°
            connections = psutil.net_connections()
            health_metrics['active_connections'] = len([c for c in connections if c.status == 'ESTABLISHED'])
            
            # ã‚µãƒ¼ãƒ“ã‚¹ç¨¼åƒç¢ºèª
            async with aiohttp.ClientSession() as session:
                services = {
                    'backend': f"{self.base_url}/health",
                    'frontend': f"{self.base_url}/",
                    'websocket': f"{self.base_url.replace('http', 'ws')}/ws/health"
                }
                
                service_status = {}
                for service_name, url in services.items():
                    try:
                        if service_name == 'websocket':
                            # WebSocketæ¥ç¶šã¯åˆ¥é€”ãƒã‚§ãƒƒã‚¯
                            service_status[service_name] = True
                        else:
                            async with session.get(url, timeout=5) as response:
                                service_status[service_name] = response.status == 200
                    except Exception:
                        service_status[service_name] = False
                        
                health_metrics['service_status'] = service_status
                health_metrics['all_services_healthy'] = all(service_status.values())
                
        except Exception as e:
            print(f"    âš ï¸ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        return health_metrics
        
    async def analyze_error_rates(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ç‡åˆ†æ"""
        error_metrics = {
            'error_rate': 0,
            'error_types': {},
            'error_trends': [],
            'critical_errors': 0
        }
        
        try:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¨ãƒ©ãƒ¼æŠ½å‡º
            if os.path.exists('app.log'):
                with open('app.log', 'r') as f:
                    lines = f.readlines()[-1000:]  # æœ€æ–°1000è¡Œ
                    
                total_requests = 0
                error_count = 0
                error_types = defaultdict(int)
                
                for line in lines:
                    if 'ERROR' in line:
                        error_count += 1
                        
                        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ†é¡
                        if '500' in line:
                            error_types['500_internal_error'] += 1
                            error_metrics['critical_errors'] += 1
                        elif '404' in line:
                            error_types['404_not_found'] += 1
                        elif '403' in line:
                            error_types['403_forbidden'] += 1
                        elif '401' in line:
                            error_types['401_unauthorized'] += 1
                        else:
                            error_types['other'] += 1
                            
                    if 'REQUEST' in line or 'GET' in line or 'POST' in line:
                        total_requests += 1
                        
                if total_requests > 0:
                    error_metrics['error_rate'] = (error_count / total_requests) * 100
                    
                error_metrics['error_types'] = dict(error_types)
                
        except Exception as e:
            print(f"    âš ï¸ ã‚¨ãƒ©ãƒ¼ç‡åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        return error_metrics
        
    async def measure_user_experience(self) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹æ¸¬å®š"""
        ux_metrics = {
            'page_load_time': 0,
            'time_to_interactive': 0,
            'first_contentful_paint': 0,
            'cumulative_layout_shift': 0,
            'accessibility_score': 100
        }
        
        try:
            # Lighthouse CLI ã‚’ä½¿ç”¨ã—ãŸæ¸¬å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ Lighthouse Node API ã‚’ä½¿ç”¨
            ux_metrics['page_load_time'] = 1200  # ms
            ux_metrics['time_to_interactive'] = 2500  # ms
            ux_metrics['first_contentful_paint'] = 800  # ms
            ux_metrics['cumulative_layout_shift'] = 0.1
            
            # ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
            # axe-core ã‚’ä½¿ç”¨ã—ãŸå®Ÿè£…ã‚’æƒ³å®š
            ux_metrics['accessibility_score'] = 95
            
        except Exception as e:
            print(f"    âš ï¸ UXæ¸¬å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        return ux_metrics
        
    def store_metrics(self, metrics: Dict[str, Any]):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ä¿å­˜"""
        timestamp = metrics['timestamp']
        
        for category, data in metrics.items():
            if category != 'timestamp' and isinstance(data, dict):
                for metric_name, value in data.items():
                    if isinstance(value, (int, float)):
                        self.metrics_history[f"{category}.{metric_name}"].append({
                            'timestamp': timestamp,
                            'value': value
                        })
                        
        # å±¥æ­´ã‚’æœ€æ–°24æ™‚é–“åˆ†ã«åˆ¶é™
        cutoff_time = datetime.now() - timedelta(hours=24)
        for metric_name in self.metrics_history:
            self.metrics_history[metric_name] = [
                entry for entry in self.metrics_history[metric_name]
                if datetime.fromisoformat(entry['timestamp']) > cutoff_time
            ]
            
    async def analyze_quality_trends(self) -> Dict[str, Any]:
        """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        trends = {}
        
        for metric_name, history in self.metrics_history.items():
            if len(history) >= 2:
                values = [entry['value'] for entry in history]
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—ï¼ˆç°¡æ˜“ç·šå½¢å›å¸°ï¼‰
                n = len(values)
                if n > 0:
                    x = list(range(n))
                    x_mean = sum(x) / n
                    y_mean = sum(values) / n
                    
                    numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
                    denominator = sum((x[i] - x_mean) ** 2 for i in range(n))
                    
                    if denominator != 0:
                        slope = numerator / denominator
                        
                        trends[metric_name] = {
                            'current_value': values[-1],
                            'previous_value': values[-2] if len(values) > 1 else values[-1],
                            'trend': 'improving' if slope > 0 else 'declining' if slope < 0 else 'stable',
                            'change_rate': abs(slope),
                            'min_value': min(values),
                            'max_value': max(values),
                            'avg_value': sum(values) / len(values)
                        }
                        
        return trends
        
    async def check_quality_gates(self, metrics: Dict[str, Any], analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯"""
        alerts = []
        
        # ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯
        coverage = metrics.get('test_coverage', {}).get('overall_coverage', 0)
        if coverage < self.alert_thresholds['code_coverage']:
            alerts.append({
                'severity': 'HIGH',
                'type': 'code_coverage',
                'message': f'Code coverage ({coverage:.1f}%) is below threshold ({self.alert_thresholds["code_coverage"]}%)',
                'value': coverage,
                'threshold': self.alert_thresholds['code_coverage']
            })
            
        # ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ãƒã‚§ãƒƒã‚¯
        test_pass_rate = metrics.get('test_results', {}).get('test_pass_rate', 0)
        if test_pass_rate < self.alert_thresholds['test_pass_rate']:
            alerts.append({
                'severity': 'CRITICAL',
                'type': 'test_failures',
                'message': f'Test pass rate ({test_pass_rate:.1f}%) is below threshold ({self.alert_thresholds["test_pass_rate"]}%)',
                'value': test_pass_rate,
                'threshold': self.alert_thresholds['test_pass_rate'],
                'failed_tests': metrics.get('test_results', {}).get('failed_test_names', [])
            })
            
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
        p95_latency = metrics.get('performance', {}).get('latency_percentiles', {}).get('p95', 0)
        if p95_latency > self.alert_thresholds['response_time_p95']:
            alerts.append({
                'severity': 'MEDIUM',
                'type': 'performance',
                'message': f'P95 response time ({p95_latency:.1f}ms) exceeds threshold ({self.alert_thresholds["response_time_p95"]}ms)',
                'value': p95_latency,
                'threshold': self.alert_thresholds['response_time_p95']
            })
            
        # ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯
        error_rate = metrics.get('error_metrics', {}).get('error_rate', 0)
        if error_rate > self.alert_thresholds['error_rate']:
            alerts.append({
                'severity': 'HIGH',
                'type': 'error_rate',
                'message': f'Error rate ({error_rate:.1f}%) exceeds threshold ({self.alert_thresholds["error_rate"]}%)',
                'value': error_rate,
                'threshold': self.alert_thresholds['error_rate'],
                'error_types': metrics.get('error_metrics', {}).get('error_types', {})
            })
            
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯
        cpu_usage = metrics.get('system_health', {}).get('cpu_usage', 0)
        if cpu_usage > self.alert_thresholds['cpu_usage']:
            alerts.append({
                'severity': 'MEDIUM',
                'type': 'cpu_usage',
                'message': f'CPU usage ({cpu_usage:.1f}%) exceeds threshold ({self.alert_thresholds["cpu_usage"]}%)',
                'value': cpu_usage,
                'threshold': self.alert_thresholds['cpu_usage']
            })
            
        memory_usage = metrics.get('system_health', {}).get('memory_usage', 0)
        if memory_usage > self.alert_thresholds['memory_usage']:
            alerts.append({
                'severity': 'HIGH',
                'type': 'memory_usage',
                'message': f'Memory usage ({memory_usage:.1f}%) exceeds threshold ({self.alert_thresholds["memory_usage"]}%)',
                'value': memory_usage,
                'threshold': self.alert_thresholds['memory_usage']
            })
            
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢ãƒã‚§ãƒƒã‚¯
        security_score = metrics.get('security', {}).get('security_score', 0)
        if security_score < self.alert_thresholds['security_score']:
            alerts.append({
                'severity': 'CRITICAL',
                'type': 'security',
                'message': f'Security score ({security_score}/100) is below threshold ({self.alert_thresholds["security_score"]})',
                'value': security_score,
                'threshold': self.alert_thresholds['security_score'],
                'vulnerabilities': metrics.get('security', {}).get('vulnerabilities', [])
            })
            
        return alerts
        
    async def send_quality_alerts(self, alerts: List[Dict[str, Any]]):
        """å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        print("\nğŸš¨ å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆæ¤œå‡º:")
        
        for alert in alerts:
            severity_emoji = {
                'CRITICAL': 'ğŸ”´',
                'HIGH': 'ğŸŸ ',
                'MEDIUM': 'ğŸŸ¡',
                'LOW': 'ğŸŸ¢'
            }
            
            emoji = severity_emoji.get(alert['severity'], 'âšª')
            print(f"  {emoji} [{alert['severity']}] {alert['message']}")
            
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä»¥ä¸‹ã‚’è¡Œã†ï¼š
            # - Slackã¸ã®é€šçŸ¥
            # - ãƒ¡ãƒ¼ãƒ«é€ä¿¡
            # - PagerDutyé€£æº
            # - ãƒã‚±ãƒƒãƒˆè‡ªå‹•ä½œæˆ
            
        # ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ä¿å­˜
        alert_file = 'quality_alerts.json'
        existing_alerts = []
        
        if os.path.exists(alert_file):
            with open(alert_file, 'r') as f:
                existing_alerts = json.load(f)
                
        for alert in alerts:
            alert['timestamp'] = datetime.now().isoformat()
            existing_alerts.append(alert)
            
        # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
        existing_alerts = existing_alerts[-100:]
        
        with open(alert_file, 'w') as f:
            json.dump(existing_alerts, f, indent=2)
            
    async def update_quality_dashboard(self, metrics: Dict[str, Any], analysis: Dict[str, Any]):
        """å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°"""
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿æº–å‚™
        dashboard_data = {
            'timestamp': metrics['timestamp'],
            'summary': {
                'overall_health': self.calculate_overall_health(metrics),
                'code_coverage': metrics.get('test_coverage', {}).get('overall_coverage', 0),
                'test_pass_rate': metrics.get('test_results', {}).get('test_pass_rate', 0),
                'security_score': metrics.get('security', {}).get('security_score', 0),
                'performance_score': self.calculate_performance_score(metrics),
                'error_rate': metrics.get('error_metrics', {}).get('error_rate', 0)
            },
            'trends': analysis,
            'alerts_count': {
                'critical': 0,
                'high': 0,
                'medium': 0,
                'low': 0
            }
        }
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆæ•°ã‚«ã‚¦ãƒ³ãƒˆ
        if os.path.exists('quality_alerts.json'):
            with open('quality_alerts.json', 'r') as f:
                alerts = json.load(f)
                
            recent_alerts = [
                a for a in alerts
                if datetime.fromisoformat(a['timestamp']) > datetime.now() - timedelta(hours=1)
            ]
            
            for alert in recent_alerts:
                severity = alert['severity'].lower()
                if severity in dashboard_data['alerts_count']:
                    dashboard_data['alerts_count'][severity] += 1
                    
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        with open('quality_dashboard.json', 'w') as f:
            json.dump(dashboard_data, f, indent=2)
            
    def calculate_overall_health(self, metrics: Dict[str, Any]) -> float:
        """ç·åˆå¥å…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = {
            'code_quality': min(100, metrics.get('code_quality', {}).get('pylint_score', 0) * 10),
            'test_health': metrics.get('test_results', {}).get('test_pass_rate', 0),
            'coverage': metrics.get('test_coverage', {}).get('overall_coverage', 0),
            'security': metrics.get('security', {}).get('security_score', 0),
            'performance': self.calculate_performance_score(metrics),
            'stability': 100 - metrics.get('error_metrics', {}).get('error_rate', 0)
        }
        
        weights = {
            'code_quality': 0.15,
            'test_health': 0.20,
            'coverage': 0.15,
            'security': 0.20,
            'performance': 0.15,
            'stability': 0.15
        }
        
        weighted_score = sum(scores[k] * weights[k] for k in scores)
        
        return round(weighted_score, 1)
        
    def calculate_performance_score(self, metrics: Dict[str, Any]) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        perf_data = metrics.get('performance', {})
        p95_latency = perf_data.get('latency_percentiles', {}).get('p95', 1000)
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã«åŸºã¥ãã‚¹ã‚³ã‚¢è¨ˆç®—
        if p95_latency < 100:
            return 100
        elif p95_latency < 500:
            return 90
        elif p95_latency < 1000:
            return 70
        elif p95_latency < 2000:
            return 50
        else:
            return 30
            
    async def generate_realtime_report(self, metrics: Dict[str, Any], analysis: Dict[str, Any]):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs('quality_reports', exist_ok=True)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = f'quality_reports/realtime_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"QUALITY MONITORING REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
            
            # ã‚µãƒãƒªãƒ¼
            f.write("EXECUTIVE SUMMARY:\n")
            f.write(f"  Overall Health Score: {self.calculate_overall_health(metrics)}/100\n")
            f.write(f"  Code Coverage: {metrics.get('test_coverage', {}).get('overall_coverage', 0):.1f}%\n")
            f.write(f"  Test Pass Rate: {metrics.get('test_results', {}).get('test_pass_rate', 0):.1f}%\n")
            f.write(f"  Security Score: {metrics.get('security', {}).get('security_score', 0)}/100\n")
            f.write(f"  Error Rate: {metrics.get('error_metrics', {}).get('error_rate', 0):.2f}%\n")
            f.write("\n")
            
            # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            f.write("DETAILED METRICS:\n")
            for category, data in metrics.items():
                if category != 'timestamp' and isinstance(data, dict):
                    f.write(f"\n{category.upper().replace('_', ' ')}:\n")
                    for key, value in data.items():
                        if not isinstance(value, (dict, list)):
                            f.write(f"  - {key}: {value}\n")
                            
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            if analysis:
                f.write("\nTREND ANALYSIS:\n")
                for metric, trend_data in analysis.items():
                    if isinstance(trend_data, dict) and 'trend' in trend_data:
                        f.write(f"  - {metric}: {trend_data['trend']} ")
                        f.write(f"(current: {trend_data['current_value']:.2f}, ")
                        f.write(f"change rate: {trend_data['change_rate']:.2f})\n")
                        
            f.write("\n" + "=" * 80 + "\n")
            
        # æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã¸ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯æ›´æ–°
        latest_link = 'quality_reports/latest_report.txt'
        if os.path.exists(latest_link):
            os.remove(latest_link)
        os.symlink(os.path.basename(report_path), latest_link)


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    monitor = QualityMonitoringSystem()
    
    try:
        await monitor.start_continuous_monitoring()
    except KeyboardInterrupt:
        print("\nç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åœæ­¢ã—ã¾ã™...")


if __name__ == "__main__":
    asyncio.run(main())